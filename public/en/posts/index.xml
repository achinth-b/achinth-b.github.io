<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>posts on achinth</title>
    <link>http://localhost:1313/en/posts/</link>
    <description>Recent content in posts on achinth</description>
    <generator>Hugo</generator>
    <language>en-CA</language>
    <lastBuildDate>Sat, 28 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/en/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>on american exceptionalism, sriram and immigration</title>
      <link>http://localhost:1313/en/posts/exceptionalism/</link>
      <pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/en/posts/exceptionalism/</guid>
      <description>&lt;p&gt;I was enjoying solitude on Christmas eve, when I came across this tweet:&lt;/p&gt;&#xA;</description>
    </item>
    <item>
      <title>[PART 1] circuit discovery: what the hell is it?</title>
      <link>http://localhost:1313/en/posts/circuit-discovery-1/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/en/posts/circuit-discovery-1/</guid>
      <description>&lt;p&gt;&lt;em&gt;This is part one of a series of posts about learning biased circuits in vision/language models. I want to publish a paper to ICML at the end of this.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Like most people, I don&amp;rsquo;t know anything about this entire sub-field.&#xA;It&amp;rsquo;s exhausting, humbling and most of all - exciting to learn something new about how we can understand generative models.&lt;/p&gt;&#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.perplexity.ai/&#34;&gt;perplexity&lt;/a&gt;, I get to learn what this subfield is. Let&amp;rsquo;s get started.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;what-is-circuit-discovery-and-how-did-we-get-here&#34;&gt;&#xA;  what is circuit discovery and how did we get here?&lt;span class=&#34;heading__anchor&#34;&gt; &lt;a href=&#34;#what-is-circuit-discovery-and-how-did-we-get-here&#34;&gt;#&lt;/a&gt;&lt;/span&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Circuit discovery is a technique from mechanistic interpretability which aims to identify and analyze the internals of generative models.&#xA;The abstraction tis technique interfaces with is on identifying &amp;lsquo;circuits&amp;rsquo; - or more precisely, pathways through which transformers process information.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading&#34; id=&#34;contextual-decomposition&#34;&gt;&#xA;  contextual decomposition&lt;span class=&#34;heading__anchor&#34;&gt; &lt;a href=&#34;#contextual-decomposition&#34;&gt;#&lt;/a&gt;&lt;/span&gt;&#xA;&lt;/h3&gt;&lt;p&gt;One method method of circuit discovery in transformers is &amp;lsquo;contextual decomposition (CD) for transformers&amp;rsquo;. This works by:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;recursive computation: CD recursively computes the contributions of all nodes (eg. attention heads, neurons) in the model&amp;rsquo;s computational graph. This means that for each output, the method assesses how much each component controbutes to the output based on its interactions with other components.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;equations: this method uses a set of well-defined equations which isolate the contributions of one model feature from another. This allows for a clear understanding of how specific features affect the overall behaviour of the model.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;pruning: after the contributions have been calculated, CD applies a pruning step to removes node that insignificantly impact the output. THis step simplifies the discovered circuit and focuses on the most relevant components.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 class=&#34;heading&#34; id=&#34;advantages-and-disadvantages-of-contextual-decomposition&#34;&gt;&#xA;  advantages and disadvantages of contextual decomposition&lt;span class=&#34;heading__anchor&#34;&gt; &lt;a href=&#34;#advantages-and-disadvantages-of-contextual-decomposition&#34;&gt;#&lt;/a&gt;&lt;/span&gt;&#xA;&lt;/h4&gt;&lt;p&gt;Having been evaluated over several circuit evaluation tasks such as indiret object identification and greater-than comparisons, CD has a high degree of faithfulness to the original model&amp;rsquo;s behaviour, replicating its performance for fewer nodes than competing approaches. It also doesn&amp;rsquo;t require manual crafting of examples or additional training, making it applicable across various transformer architectures.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading&#34; id=&#34;sparse-autoencoders&#34;&gt;&#xA;  sparse autoencoders&lt;span class=&#34;heading__anchor&#34;&gt; &lt;a href=&#34;#sparse-autoencoders&#34;&gt;#&lt;/a&gt;&lt;/span&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Sparse autoencoders (SAEs) are a specialized type of autoencoder which focuses on learning efficient represntations of data by enforcing sparsity in encoded representations. This works by:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;compressing input data into a lower-dimensional representation and a decoder that reconstructs the original input from this representation - with the added benefit of enforcing a sparsity constraint in the latent space by adding it to the loss function (which I just was reminded - is L1 regularization.)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;breaking down neural networks into understandable components without requiring labelled examples. Since sparsity is enforced, most noisy variables are ignored as well as irrelevant information.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
    </item>
    <item>
      <title>Hi!</title>
      <link>http://localhost:1313/en/posts/post-1/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/en/posts/post-1/</guid>
      <description>&lt;p&gt;This year, has been the darkest of my life so far. I lost my job. I lost some dear friends. I lost hope. I herniated my back. I moved back home. In the pursuit of the things I accomplished, &lt;em&gt;I forgot who I was supposed to be&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;So I forced myself to think more about what I&amp;rsquo;m doing here. I started writing my thoughts which felt disgustingly difficult at first. My self-confidence has never been lower, and if I don&amp;rsquo;t get comfortable with myself again, I will end up falling deeper into the hole I&amp;rsquo;ve been living in for the past six months.&lt;/p&gt;&#xA;&lt;p&gt;If I don&amp;rsquo;t force myself to start a blog, then I never will. Even though I have an &amp;lsquo;about&amp;rsquo; page, this is a much more in-depth breakdown of the following.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Who am I?&lt;/li&gt;&#xA;&lt;li&gt;What do I care about?&lt;/li&gt;&#xA;&lt;li&gt;What do I want to do with my life?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So, here goes.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I love statistics, mathematics, represenatation, data and computing (in that order).&lt;/li&gt;&#xA;&lt;li&gt;I am (after a long, often &lt;a href=&#34;https://nctr.ca/education/teaching-resources/residential-school-history/&#34;&gt;confusing&lt;/a&gt;, &lt;a href=&#34;https://www.publicsafety.gc.ca/cnt/rsrcs/pblctns/lssns-lrnd/index-en.aspx&#34;&gt;tumultuous&lt;/a&gt; and &lt;a href=&#34;https://www.thecanadianencyclopedia.ca/en/article/internment-of-japanese-canadians&#34;&gt;frustrating&lt;/a&gt; relationship with what it meant to be one), a proud &lt;a href=&#34;https://www.canada.ca/en/immigration-refugees-citizenship/services/new-immigrants/learn-about-canada/canadians.html&#34;&gt;Canadian&lt;/a&gt;. I spent the first seven years of my life in Singapore and had a short stint in India. I lived for a little bit in the United States too.&lt;/li&gt;&#xA;&lt;li&gt;I am a cultural Hindu of Indian heritage. I was raised in a &lt;em&gt;fairly&lt;/em&gt; orthodox &lt;a href=&#34;https://en.wikipedia.org/wiki/Smarta_tradition&#34;&gt;SmƒÅrta Brahmin&lt;/a&gt; household.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I am &lt;a href=&#34;https://en.wikipedia.org/wiki/Dvija#The_meaning_of_the_two_births&#34;&gt;twice-born&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;I was raised in a &lt;a href=&#34;https://www.hinduamerican.org/blog/4-things-about-hinduism-and-vegetarianism/&#34;&gt;vegetarian&lt;/a&gt; household.&lt;/li&gt;&#xA;&lt;li&gt;I don&amp;rsquo;t consider myself a spiritual person.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;I play ultimate frisbee. I may not be the best at it, but it&amp;rsquo;s given me so much, such as:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;how to move on from mistakes quickly&lt;/li&gt;&#xA;&lt;li&gt;weighlifting, cardio and agility training&lt;/li&gt;&#xA;&lt;li&gt;some of my closest friends, no matter where I go&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;I love playing around with machine learning models and building things on top of them, and perfecting them.&lt;/li&gt;&#xA;&lt;li&gt;I have come to the conclusion that algorithmic bias and AI safety are going to be my life project. I don&amp;rsquo;t want the issues that we have from our past and present metaphysical condition to carry to the new advancements we bring in pursuit of a better future.&lt;/li&gt;&#xA;&lt;li&gt;I am a civic market socialist democrat. I like free-market economics, and I don&amp;rsquo;t like how we got to this point in the human condition where we allowed ourselves to be exploited more. I&amp;rsquo;m interested in building a stronger, more united and resilient Canada through welfare state to raise the floor for every Canadian.&lt;/li&gt;&#xA;&lt;li&gt;I haven&amp;rsquo;t made peace with the fact that I want to do it all. Maybe I will get there someday.&lt;/li&gt;&#xA;&lt;li&gt;I love trains. I love taking the subway. I love public transit. I love riding my bike wherever it can take me. I despise cars and hate being forced to drive.&lt;/li&gt;&#xA;&lt;li&gt;I hate the McMansions that a stereotypical settled-down dream is associated with. I am a &lt;a href=&#34;https://www.housingisahumanright.org/what-is-a-yimby-hint-its-not-good/&#34;&gt;YIMBY&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
    </item>
  </channel>
</rss>
