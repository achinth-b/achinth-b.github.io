<!DOCTYPE html>
<html class="html" lang="en-CA" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>[PART 1] circuit discovery: what the hell is it? | achinth</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">

      <link rel="stylesheet" href="/css/style.min.3f8a9a853a8f9b4fab6b1688be9903c8c611de708992ebbb5b2ee74890e7ba8c.css" integrity="sha256-P4qahTqPm0&#43;raxaIvpkDyMYR3nCJkuu7Wy7nSJDnuow=" crossorigin="anonymous">

<link rel="icon" href="/favicon.ico" />

<meta name="description" content="This is part one of a series of posts about learning biased circuits in vision/language models. I want to publish a paper to ICML at the end of this.
Like most people, I don’t know anything about this entire sub-field. It’s exhausting, humbling and most of all - exciting to learn something new about how we can understand generative models.
Thanks to perplexity, I get to learn what this subfield is. Let’s get started.">
<meta property="og:url" content="https://achinth-b.github.io/en/posts/circuit-discovery-1/">
  <meta property="og:site_name" content="achinth">
  <meta property="og:title" content="[PART 1] circuit discovery: what the hell is it?">
  <meta property="og:description" content="This is part one of a series of posts about learning biased circuits in vision/language models. I want to publish a paper to ICML at the end of this.
Like most people, I don’t know anything about this entire sub-field. It’s exhausting, humbling and most of all - exciting to learn something new about how we can understand generative models.
Thanks to perplexity, I get to learn what this subfield is. Let’s get started.">
  <meta property="og:locale" content="en_CA">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-10T00:00:00+00:00">
    <meta property="article:tag" content="Circuit-Discovery">
    <meta property="article:tag" content="Mechanistic Interpretability">
    <meta property="og:image" content="https://achinth-b.github.io/favicon.ico">


  <meta itemprop="name" content="[PART 1] circuit discovery: what the hell is it?">
  <meta itemprop="description" content="This is part one of a series of posts about learning biased circuits in vision/language models. I want to publish a paper to ICML at the end of this.
Like most people, I don’t know anything about this entire sub-field. It’s exhausting, humbling and most of all - exciting to learn something new about how we can understand generative models.
Thanks to perplexity, I get to learn what this subfield is. Let’s get started.">
  <meta itemprop="datePublished" content="2024-12-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-12-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="420">
  <meta itemprop="image" content="https://achinth-b.github.io/favicon.ico">
  <meta itemprop="keywords" content="Circuit-Discovery,Mechanistic Interpretability">


</head>
<body class="body">
  <header class="header">
    <h1>achinth</h1>
  <nav class="menu language">
    <ul class="menu__list language__list">
    <li class="menu__item">
      <a class="menu__link" href="/en/">home</a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/en/about/">about</a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/en/experience/">experience</a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/en/tags/">tags</a>
    </li>
</ul>
  </nav>

  </header>
  <main class="main">
    
  <h1>[PART 1] circuit discovery: what the hell is it?</h1>
  <time class="published-date" datetime="2024-12-10T00:00:00&#43;00:00">2024-12-10</time>
  

  




  <details class="toc">
    <summary class="toc__summary">Table of Contents</summary>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-circuit-discovery-and-how-did-we-get-here">what is circuit discovery and how did we get here?</a>
      <ul>
        <li><a href="#contextual-decomposition">contextual decomposition</a>
          <ul>
            <li><a href="#advantages-and-disadvantages-of-contextual-decomposition">advantages and disadvantages of contextual decomposition</a></li>
          </ul>
        </li>
        <li><a href="#sparse-autoencoders">sparse autoencoders</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </details>

  <p><em>This is part one of a series of posts about learning biased circuits in vision/language models. I want to publish a paper to ICML at the end of this.</em></p>
<p>Like most people, I don&rsquo;t know anything about this entire sub-field.
It&rsquo;s exhausting, humbling and most of all - exciting to learn something new about how we can understand generative models.</p>
<p>Thanks to <a href="https://www.perplexity.ai/">perplexity</a>, I get to learn what this subfield is. Let&rsquo;s get started.</p>
<h2 class="heading" id="what-is-circuit-discovery-and-how-did-we-get-here">
  what is circuit discovery and how did we get here?<span class="heading__anchor"> <a href="#what-is-circuit-discovery-and-how-did-we-get-here">#</a></span>
</h2><p>Circuit discovery is a technique from mechanistic interpretability which aims to identify and analyze the internals of generative models.
The abstraction tis technique interfaces with is on identifying &lsquo;circuits&rsquo; - or more precisely, pathways through which transformers process information.</p>
<h3 class="heading" id="contextual-decomposition">
  contextual decomposition<span class="heading__anchor"> <a href="#contextual-decomposition">#</a></span>
</h3><p>One method method of circuit discovery in transformers is &lsquo;contextual decomposition (CD) for transformers&rsquo;. This works by:</p>
<ul>
<li>
<p>recursive computation: CD recursively computes the contributions of all nodes (eg. attention heads, neurons) in the model&rsquo;s computational graph. This means that for each output, the method assesses how much each component controbutes to the output based on its interactions with other components.</p>
</li>
<li>
<p>equations: this method uses a set of well-defined equations which isolate the contributions of one model feature from another. This allows for a clear understanding of how specific features affect the overall behaviour of the model.</p>
</li>
<li>
<p>pruning: after the contributions have been calculated, CD applies a pruning step to removes node that insignificantly impact the output. THis step simplifies the discovered circuit and focuses on the most relevant components.</p>
</li>
</ul>
<h4 class="heading" id="advantages-and-disadvantages-of-contextual-decomposition">
  advantages and disadvantages of contextual decomposition<span class="heading__anchor"> <a href="#advantages-and-disadvantages-of-contextual-decomposition">#</a></span>
</h4><p>Having been evaluated over several circuit evaluation tasks such as indiret object identification and greater-than comparisons, CD has a high degree of faithfulness to the original model&rsquo;s behaviour, replicating its performance for fewer nodes than competing approaches. It also doesn&rsquo;t require manual crafting of examples or additional training, making it applicable across various transformer architectures.</p>
<h3 class="heading" id="sparse-autoencoders">
  sparse autoencoders<span class="heading__anchor"> <a href="#sparse-autoencoders">#</a></span>
</h3><p>Sparse autoencoders (SAEs) are a specialized type of autoencoder which focuses on learning efficient represntations of data by enforcing sparsity in encoded representations. This works by:</p>
<ul>
<li>
<p>compressing input data into a lower-dimensional representation and a decoder that reconstructs the original input from this representation - with the added benefit of enforcing a sparsity constraint in the latent space by adding it to the loss function (which I just was reminded - is L1 regularization.)</p>
</li>
<li>
<p>breaking down neural networks into understandable components without requiring labelled examples. Since sparsity is enforced, most noisy variables are ignored as well as irrelevant information.</p>
</li>
</ul>

  
  <div>tags: <a href="/en/tags/circuit-discovery/">Circuit-Discovery</a>,  <a href="/en/tags/mechanistic-interpretability/">Mechanistic Interpretability</a>
  </div>
  
  
  <nav class="page-nav">
      <a class="page-nav__previous-link" href="/en/posts/exceptionalism/">Prev: on american exceptionalism, sriram and immigration</a>
      <a class="page-nav__next-link" href="/en/posts/post-1/">Next: Hi!</a>
  </nav>

  

  </main>
  <footer class="footer">
    
<p class="footer__copyright-notice">&copy; 2025 Achinth Bharadwaj</p>
<div style="text-align: center; margin-bottom: 1rem;">
  <a href="https://michaelfromorg.github.io/ubc-webring/" target="_blank" rel="noopener noreferrer">
    <img src="/ubc-coa.svg" alt="UBC Webring" width="36" height="50">
  </a>
</div>

  </footer>
  
</body>
</html>
